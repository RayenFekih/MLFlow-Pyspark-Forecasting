{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engineering.engineering import engineerFeatures\n",
    "from utils.utils import gapfilling, serialize\n",
    "from modelling.model_utils import splitData, prepare_data, train_model, MLFlow_train_model\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/05 22:42:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]: move config dictionnaries to a json config file\n",
    "features_config = {\n",
    "\n",
    "    \"discount_rate\":{},\n",
    "    \"promoted_percent\":{\"promoted_hierarchy\": \"sku\", \"group_key\":\"subclass\"},\n",
    "    \"week_of_year\":{},\n",
    "    \"promo_category\":{},\n",
    "}\n",
    "\n",
    "model_config={\n",
    "    \"model\": \"xgboost\",\n",
    "    \"params\": {},\n",
    "    \"hierarchy_columns\": [\"sku\", \"subclass\", \"store_id\", \"region_id\"],\n",
    "    \"target\": \"units\",\n",
    "    \"train_startDate\": \"2018-01-01\",\n",
    "    \"train_endDate\": \"2020-01-01\",\n",
    "    \"inference_startDate\": \"2019-11-01\",\n",
    "    \"inference_endDate\": \"2020-12-21\",\n",
    "}\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union transcational data\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"week_index\", StringType(), True),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"promo_cat\", StringType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"store_id\", StringType(), True)],\n",
    ")\n",
    "\n",
    "transactions = spark.read.csv(\n",
    "    \"data/transactions_*.csv\", \n",
    "    schema=schema,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "customers = spark.read.csv(\n",
    "    \"data/customers.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "calendar = spark.read.csv(\n",
    "    \"data/calendar.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"data/products.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "stores = spark.read.csv(\n",
    "    \"data/stores.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding location hierarchy for customers\n",
    "\n",
    "customers = customers.select(\n",
    "    F.col(\"customer_id\").cast(\"string\"), \n",
    "    F.col(\"store_pref\").cast(\"string\").alias(\"store_id\")\n",
    ")\n",
    "\n",
    "stores = stores.select(\n",
    "    F.col(\"store_id\").cast(\"string\"), \n",
    "    F.col(\"store_region\").cast(\"string\").alias(\"region_id\")\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.select(\n",
    "    F.col(\"prod_id\").cast(\"string\").alias(\"sku\"),\n",
    "    F.col(\"prod_subclass\").cast(\"string\").alias(\"subclass\"),\n",
    "    F.col(\"prod_class\").cast(\"string\").alias(\"class\"),\n",
    "    F.col(\"prod_dept\").cast(\"string\").alias(\"dept\"),\n",
    "    F.col(\"prod_base_price\").cast(\"float\").alias(\"base_price\"),\n",
    ").dropDuplicates()\n",
    "\n",
    "products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily calendar -> weekly calendar\n",
    "weekly_calendar = calendar.where(\n",
    "    F.col(\"day_of_week\")==\"0\"\n",
    ").select(\n",
    "    F.to_date(F.col(\"calendar_day\"),\"MM-dd-yy\").alias(\"date\")\n",
    ").distinct(\n",
    ").sort(\n",
    "    F.col(\"date\").asc()\n",
    ").withColumn(\n",
    "    \"week_index\", F.monotonically_increasing_id()\n",
    ").select(\n",
    "    F.col(\"week_index\").cast(\"string\"),\n",
    "    F.col(\"date\")\n",
    ")\n",
    "weekly_calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hierarchies\n",
    "demand_data = transactions.groupby(\n",
    "    \"sku\", \"store_id\", \"week_index\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"units\"),\n",
    "    F.first(\"promo_cat\").alias(\"promo_cat\"),\n",
    "    F.max(\"discount\").alias(\"discount\"),\n",
    ").join(\n",
    "    weekly_calendar, on=[\"week_index\"], how=\"inner\"\n",
    ").drop(\"week_index\")\n",
    "\n",
    "serialize(spark, demand_data, path + \"demand_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_data = spark.read.parquet(path + \"demand_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_filled_data = gapfilling(demand_data, date_column=\"date\", product_column=\"sku\", location_column=\"store_id\")\n",
    "\n",
    "serialize(spark, sales_filled_data, path + \"sales_filled_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding product and location hierarchies to demand data\n",
    "demand_data = spark.read.parquet(path + \"sales_filled_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_data = demand_data.join(\n",
    "    stores, on=\"store_id\", how=\"inner\"\n",
    ").join(\n",
    "    products, on=\"sku\", how=\"inner\"\n",
    ")\n",
    "\n",
    "serialize(spark, sales_data, path + \"sales_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = spark.read.parquet(path + \"sales_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "engineered_data = engineerFeatures(\n",
    "    data=sales_data,\n",
    "    config=features_config\n",
    ")\n",
    "\n",
    "serialize(spark, engineered_data, path + \"engineered_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_data = spark.read.parquet(path + \"engineered_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "engineered_data = engineered_data.where(F.col(\"sku\")<400) # Just to reduce the size of the data for less memory consumption\n",
    "\n",
    "train_data, test_data = splitData(\n",
    "        data=engineered_data,\n",
    "        model_config=model_config,\n",
    "        features_config=features_config\n",
    "    )\n",
    "\n",
    "serialize(spark, train_data, path + \"train_data.parquet\").show(5)\n",
    "serialize(spark, test_data, path + \"test_data.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-05 22:43:12.060061] One-hot encoding the train dataframe\n",
      "[2023-01-05 22:53:07.483322] Transforming the train one-hot encoded data into a CSR matrix\n",
      "[2023-01-05 22:54:36.375337] One-hot encoding the test dataframe\n",
      "[2023-01-05 22:54:50.824702] Transforming the test one-hot encoded data into a CSR matrix\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_parquet(f\"data/train_data.parquet\", \"pyarrow\")\n",
    "test_data = pd.read_parquet(f\"data/test_data.parquet\", \"pyarrow\")\n",
    "\n",
    "# preparing the training data\n",
    "X_train_ohe_sparse, y_train = prepare_data(\n",
    "    train_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"train\",\n",
    ")\n",
    "\n",
    "# preparing the inferencing data\n",
    "X_test_ohe_sparse, y_test = prepare_data(\n",
    "    test_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-05 22:55:02.872796] Loading the lgbm model\n",
      "[2023-01-05 22:55:02.873179] Fitting the lgbm model\n",
      "[2023-01-05 22:55:15.723334] Generating predictions\n",
      "[2023-01-05 22:55:16.966375] mean_squared_error_ =0.12101342410404359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00042651, -0.00042651, -0.00042651, ..., -0.00040733,\n",
       "       -0.00040733, -0.00040733])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal run\n",
    "train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/01/01 13:14:16 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-01 13:14:17.924195] Logged data and model in run 9c508d0dc958443683498caaf3851e93\n",
      "[2023-01-01 13:14:17.924471] [Run 9c508d0dc958443683498caaf3851e93]: Loading the xgboost model\n",
      "[2023-01-01 13:14:17.924578] [Run 9c508d0dc958443683498caaf3851e93]: Fitting the xgboost model\n",
      "[2023-01-01 13:14:25.180253] [Run 9c508d0dc958443683498caaf3851e93]: Generating predictions\n",
      "[2023-01-01 13:14:25.704440] [Run 9c508d0dc958443683498caaf3851e93]: mean_squared_error_=0.13993235249196243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-01 13:14:31.143475] Logged data and model in run 3fbc42fcaf704286ab8858a25823d2b6\n",
      "[2023-01-01 13:14:31.143645] [Run 3fbc42fcaf704286ab8858a25823d2b6]: Loading the xgboost model\n",
      "[2023-01-01 13:14:31.143723] [Run 3fbc42fcaf704286ab8858a25823d2b6]: Fitting the xgboost model\n",
      "[2023-01-01 13:14:39.110172] [Run 3fbc42fcaf704286ab8858a25823d2b6]: Generating predictions\n",
      "[2023-01-01 13:14:39.655306] [Run 3fbc42fcaf704286ab8858a25823d2b6]: mean_squared_error_=0.47039567296478074\n"
     ]
    }
   ],
   "source": [
    "# MLFlow run\n",
    "\n",
    "max_depth_list = [3,4]\n",
    "learning_rate_list = [0.1, 0.001]\n",
    "n_estimators_list = [20, 25]\n",
    "\n",
    "for max_depth, learning_rate, n_estimators in zip(max_depth_list, learning_rate_list, n_estimators_list):\n",
    "    model_params = {\"max_depth\":max_depth, \"learning_rate\":learning_rate, \"n_estimators\":n_estimators}\n",
    "    MLFlow_train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-01 13:23:52.822766] Loading the pretrained xgboost model in run 9c508d0dc958443683498caaf3851e93\n",
      "[2023-01-01 13:23:53.398545] Generating predictions\n",
      "[2023-01-01 13:23:53.846542] mean_squared_error_ =0.13993235249196243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.06079819, 0.06079819, 0.06079819, ..., 0.06079819, 0.06079819,\n",
       "       0.06079819], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 14:25:45 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3695768 ms exceeds timeout 120000 ms\n",
      "23/01/01 14:25:46 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config, run_id=\"9c508d0dc958443683498caaf3851e93\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-31 21:12:59 +0000] [11766] [INFO] Starting gunicorn 20.1.0\n",
      "[2022-12-31 21:12:59 +0000] [11766] [INFO] Listening at: http://127.0.0.1:5000 (11766)\n",
      "[2022-12-31 21:12:59 +0000] [11766] [INFO] Using worker: sync\n",
      "[2022-12-31 21:12:59 +0000] [11767] [INFO] Booting worker with pid: 11767\n",
      "[2022-12-31 21:12:59 +0000] [11768] [INFO] Booting worker with pid: 11768\n",
      "[2022-12-31 21:12:59 +0000] [11769] [INFO] Booting worker with pid: 11769\n",
      "[2022-12-31 21:12:59 +0000] [11770] [INFO] Booting worker with pid: 11770\n",
      "^C\n",
      "[2022-12-31 21:13:02 +0000] [11766] [INFO] Handling signal: int\n",
      "[2022-12-31 21:13:02 +0000] [11769] [INFO] Worker exiting (pid: 11769)\n",
      "[2022-12-31 21:13:02 +0000] [11768] [INFO] Worker exiting (pid: 11768)\n",
      "[2022-12-31 21:13:02 +0000] [11767] [INFO] Worker exiting (pid: 11767)\n",
      "[2022-12-31 21:13:02 +0000] [11770] [INFO] Worker exiting (pid: 11770)\n"
     ]
    }
   ],
   "source": [
    "! mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
