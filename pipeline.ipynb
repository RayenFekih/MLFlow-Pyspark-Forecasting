{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engineering.engineering import engineerFeatures\n",
    "from utils.utils import gapfilling, serialize\n",
    "from modelling.model_utils import splitData, prepare_data, train_model, MLFlow_train_model\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/30 15:26:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]: move config dictionnaries to a json config file\n",
    "features_config = {\n",
    "\n",
    "    \"discount_rate\":{},\n",
    "    \"promoted_percent\":{\"promoted_hierarchy\": \"sku\", \"group_key\":\"subclass\"},\n",
    "    \"week_of_year\":{},\n",
    "    \"promo_category\":{},\n",
    "}\n",
    "\n",
    "model_config={\n",
    "    \"model\": \"xgboost\",\n",
    "    \"params\": {},\n",
    "    \"hierarchy_columns\": [\"sku\", \"subclass\", \"store_id\", \"region_id\"],\n",
    "    \"target\": \"units\",\n",
    "    \"train_startDate\": \"2018-01-01\",\n",
    "    \"train_endDate\": \"2020-01-01\",\n",
    "    \"inference_startDate\": \"2019-11-01\",\n",
    "    \"inference_endDate\": \"2020-12-21\",\n",
    "}\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Union transcational data\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"week_index\", StringType(), True),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"promo_cat\", StringType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"store_id\", StringType(), True)],\n",
    ")\n",
    "\n",
    "transactions = spark.read.csv(\n",
    "    \"data/transactions_*.csv\", \n",
    "    schema=schema,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "customers = spark.read.csv(\n",
    "    \"data/customers.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "calendar = spark.read.csv(\n",
    "    \"data/calendar.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"data/products.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "stores = spark.read.csv(\n",
    "    \"data/stores.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding location hierarchy for customers\n",
    "\n",
    "customers = customers.select(\n",
    "    F.col(\"customer_id\").cast(\"string\"), \n",
    "    F.col(\"store_pref\").cast(\"string\").alias(\"store_id\")\n",
    ")\n",
    "\n",
    "stores = stores.select(\n",
    "    F.col(\"store_id\").cast(\"string\"), \n",
    "    F.col(\"store_region\").cast(\"string\").alias(\"region_id\")\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+----+----------+\n",
      "|sku|subclass|class|dept|base_price|\n",
      "+---+--------+-----+----+----------+\n",
      "|271|       0|    0|   7|    142.86|\n",
      "|289|       0|    0|   3|      20.0|\n",
      "|369|       0|    0|   2|     16.67|\n",
      "|410|       1|    1|  14|    525.33|\n",
      "|559|       0|    0|   4|     48.57|\n",
      "+---+--------+-----+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = products.select(\n",
    "    F.col(\"prod_id\").cast(\"string\").alias(\"sku\"),\n",
    "    F.col(\"prod_subclass\").cast(\"string\").alias(\"subclass\"),\n",
    "    F.col(\"prod_class\").cast(\"string\").alias(\"class\"),\n",
    "    F.col(\"prod_dept\").cast(\"string\").alias(\"dept\"),\n",
    "    F.col(\"prod_base_price\").cast(\"float\").alias(\"base_price\"),\n",
    ").dropDuplicates()\n",
    "\n",
    "products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|week_index|      date|\n",
      "+----------+----------+\n",
      "|         0|2018-01-01|\n",
      "|         1|2018-01-08|\n",
      "|         2|2018-01-15|\n",
      "|         3|2018-01-22|\n",
      "|         4|2018-01-29|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# daily calendar -> weekly calendar\n",
    "weekly_calendar = calendar.where(\n",
    "    F.col(\"day_of_week\")==\"0\"\n",
    ").select(\n",
    "    F.to_date(F.col(\"calendar_day\"),\"MM-dd-yy\").alias(\"date\")\n",
    ").distinct(\n",
    ").sort(\n",
    "    F.col(\"date\").asc()\n",
    ").withColumn(\n",
    "    \"week_index\", F.monotonically_increasing_id()\n",
    ").select(\n",
    "    F.col(\"week_index\").cast(\"string\"),\n",
    "    F.col(\"date\")\n",
    ")\n",
    "weekly_calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:44:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:44:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:44:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+--------+----------+\n",
      "|sku|store_id|units|promo_cat|discount|      date|\n",
      "+---+--------+-----+---------+--------+----------+\n",
      "|  0|      87|    1|      nan|    null|2020-02-03|\n",
      "|100|      83|    1|      nan|    null|2020-02-03|\n",
      "|101|      24|    1|      nan|    null|2020-02-03|\n",
      "|101|      55|    1|      nan|    null|2020-02-03|\n",
      "|102|      40|    1|      nan|    null|2020-02-03|\n",
      "+---+--------+-----+---------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add hierarchies\n",
    "demand_data = transactions.groupby(\n",
    "    \"sku\", \"store_id\", \"week_index\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"units\"),\n",
    "    F.first(\"promo_cat\").alias(\"promo_cat\"),\n",
    "    F.max(\"discount\").alias(\"discount\"),\n",
    ").join(\n",
    "    weekly_calendar, on=[\"week_index\"], how=\"inner\"\n",
    ").drop(\"week_index\")\n",
    "\n",
    "serialize(spark, demand_data, path + \"demand_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:44:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:44:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:45:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+-----+---------+--------+\n",
      "|      date|sku|store_id|units|promo_cat|discount|\n",
      "+----------+---+--------+-----+---------+--------+\n",
      "|2018-01-01|  0|      10|    0|     null|    null|\n",
      "|2018-01-01|  0|      20|    0|     null|    null|\n",
      "|2018-01-01|  0|      29|    0|     null|    null|\n",
      "|2018-01-01|  0|      34|    0|     null|    null|\n",
      "|2018-01-01|  0|      41|    0|     null|    null|\n",
      "+----------+---+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demand_data = spark.read.parquet(path + \"demand_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_filled_data = gapfilling(demand_data, date_column=\"date\", product_column=\"sku\", location_column=\"store_id\")\n",
    "\n",
    "serialize(spark, sales_filled_data, path + \"sales_filled_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:45:25 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:45:25 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:45:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:45:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "|sku|store_id|      date|units|promo_cat|discount|region_id|subclass|class|dept|base_price|\n",
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "|102|      11|2018-01-01|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "|102|       1|2018-01-22|    0|     null|    null|        2|       0|    0|   9|     164.0|\n",
      "|102|      11|2018-01-15|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "|102|       1|2018-02-26|    0|     null|    null|        2|       0|    0|   9|     164.0|\n",
      "|102|      11|2018-03-12|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding product and location hierarchies to demand data\n",
    "demand_data = spark.read.parquet(path + \"sales_filled_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_data = demand_data.join(\n",
    "    stores, on=\"store_id\", how=\"inner\"\n",
    ").join(\n",
    "    products, on=\"sku\", how=\"inner\"\n",
    ")\n",
    "\n",
    "serialize(spark, sales_data, path + \"sales_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "|subclass|sku|store_id|      date|units|promo_cat|discount|region_id|class|dept|base_price|_discount_rate|_promoted_sku_subclass|_week_of_year|_promo_category|\n",
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "|       0|102|      11|2018-01-01|    0|     null|    null|        1|    0|   9|     164.0|           1.0|                   296|            1|           null|\n",
      "|       0|358|      41|2018-03-26|    0|     null|    null|        1|    0|   2|      15.0|           1.0|                   296|           13|           null|\n",
      "|       0|102|       1|2018-01-22|    0|     null|    null|        2|    0|   9|     164.0|           1.0|                   296|            4|           null|\n",
      "|       0|358|      37|2018-06-18|    0|     null|    null|        4|    0|   2|      15.0|           1.0|                   296|           25|           null|\n",
      "|       0|102|      11|2018-01-15|    0|     null|    null|        1|    0|   9|     164.0|           1.0|                   296|            3|           null|\n",
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = spark.read.parquet(path + \"sales_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "engineered_data = engineerFeatures(\n",
    "    data=sales_data,\n",
    "    config=features_config\n",
    ")\n",
    "\n",
    "serialize(spark, engineered_data, path + \"engineered_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "|sku|subclass|store_id|region_id|_week_of_year|_promo_category|_discount_rate|_promoted_sku_subclass|units|\n",
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "|118|       1|      18|        1|            8|           null|           1.0|                   290|    0|\n",
      "|377|       1|      11|        1|           22|            nan|           1.0|                   290|    1|\n",
      "|118|       1|      24|        2|            9|           null|           1.0|                   290|    0|\n",
      "|377|       1|      24|        2|            6|           null|           1.0|                   290|    0|\n",
      "|118|       1|      18|        1|           12|           null|           1.0|                   290|    0|\n",
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "|sku|subclass|store_id|region_id|_week_of_year|_promo_category|_discount_rate|_promoted_sku_subclass|units|\n",
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "|377|       1|      11|        1|           48|           null|           1.0|                   290|    0|\n",
      "|377|       1|      11|        1|            5|            nan|           1.0|                   290|    1|\n",
      "|377|       1|      11|        1|            6|            nan|           1.0|                   290|    1|\n",
      "|377|       1|      11|        1|            7|           null|           1.0|                   290|    0|\n",
      "|377|       1|      11|        1|           12|           null|           1.0|                   290|    0|\n",
      "+---+--------+--------+---------+-------------+---------------+--------------+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engineered_data = spark.read.parquet(path + \"engineered_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "engineered_data = engineered_data.where(F.col(\"sku\")<400) # Just to reduce the size of the data for less memory consumption\n",
    "\n",
    "train_data, test_data = splitData(\n",
    "        data=engineered_data,\n",
    "        model_config=model_config,\n",
    "        features_config=features_config\n",
    "    )\n",
    "\n",
    "serialize(spark, train_data, path + \"train_data.parquet\").show(5)\n",
    "serialize(spark, test_data, path + \"test_data.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-30 15:26:31.081387] One-hot encoding the train dataframe\n",
      "[2022-12-30 15:26:46.990414] Transforming the train one-hot encoded data into a CSR matrix\n",
      "[2022-12-30 15:27:01.721415] One-hot encoding the test dataframe\n",
      "[2022-12-30 15:27:10.983409] Transforming the test one-hot encoded data into a CSR matrix\n",
      "[2022-12-30 15:27:18.988850] Loading the xgboost model\n",
      "[2022-12-30 15:27:18.989033] Fitting the xgboost model\n",
      "[2022-12-30 15:28:03.564492] Generating predictions\n",
      "[2022-12-30 15:28:06.180636] mean_squared_error_ =0.11511720835514745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00140893, -0.00140893, -0.00054343, ...,  0.00044674,\n",
       "        0.00056613,  0.0083237 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_parquet(f\"data/train_data.parquet\", \"pyarrow\")\n",
    "test_data = pd.read_parquet(f\"data/test_data.parquet\", \"pyarrow\")\n",
    "\n",
    "# preparing the training data\n",
    "X_train_ohe_sparse, y_train = prepare_data(\n",
    "    train_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"train\",\n",
    ")\n",
    "\n",
    "# preparing the inferencing data\n",
    "X_test_ohe_sparse, y_test = prepare_data(\n",
    "    test_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"test\",\n",
    ")\n",
    "\n",
    "train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/12/30 15:28:06 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-30 15:28:07.130725] Logged data and model in run 554c32dc985049d9a0c20f4589d5956e\n",
      "[2022-12-30 15:28:07.130880] [Run 554c32dc985049d9a0c20f4589d5956e]: Loading the xgboost model\n",
      "[2022-12-30 15:28:07.130942] [Run 554c32dc985049d9a0c20f4589d5956e]: Fitting the xgboost model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/12/30 15:28:17 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-30 15:28:17.279357] [Run 554c32dc985049d9a0c20f4589d5956e]: Generating predictions\n",
      "[2022-12-30 15:28:17.641315] [Run 554c32dc985049d9a0c20f4589d5956e]: mean_squared_error_=0.13993235249196243\n",
      "[2022-12-30 15:28:20.348015] Logged data and model in run c78ca2c373904ba89a7eb3d7ab9d4da0\n",
      "[2022-12-30 15:28:20.348141] [Run c78ca2c373904ba89a7eb3d7ab9d4da0]: Loading the xgboost model\n",
      "[2022-12-30 15:28:20.348186] [Run c78ca2c373904ba89a7eb3d7ab9d4da0]: Fitting the xgboost model\n",
      "[2022-12-30 15:28:32.317822] [Run c78ca2c373904ba89a7eb3d7ab9d4da0]: Generating predictions\n",
      "[2022-12-30 15:28:32.855430] [Run c78ca2c373904ba89a7eb3d7ab9d4da0]: mean_squared_error_=0.47039567296478074\n"
     ]
    }
   ],
   "source": [
    "max_depth_list = [3,4]\n",
    "learning_rate_list = [0.1, 0.001]\n",
    "n_estimators_list = [20, 25]\n",
    "\n",
    "for max_depth, learning_rate, n_estimators in zip(max_depth_list, learning_rate_list, n_estimators_list):\n",
    "    model_params = {\"max_depth\":max_depth, \"learning_rate\":learning_rate, \"n_estimators\":n_estimators}\n",
    "    MLFlow_train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-30 15:36:21 +0000] [8156] [INFO] Starting gunicorn 20.1.0\n",
      "[2022-12-30 15:36:21 +0000] [8156] [INFO] Listening at: http://127.0.0.1:5000 (8156)\n",
      "[2022-12-30 15:36:21 +0000] [8156] [INFO] Using worker: sync\n",
      "[2022-12-30 15:36:21 +0000] [8157] [INFO] Booting worker with pid: 8157\n",
      "[2022-12-30 15:36:21 +0000] [8159] [INFO] Booting worker with pid: 8159\n",
      "[2022-12-30 15:36:21 +0000] [8162] [INFO] Booting worker with pid: 8162\n",
      "[2022-12-30 15:36:21 +0000] [8163] [INFO] Booting worker with pid: 8163\n",
      "^C\n",
      "[2022-12-30 15:38:31 +0000] [8156] [INFO] Handling signal: int\n",
      "[2022-12-30 15:38:31 +0000] [8159] [INFO] Worker exiting (pid: 8159)\n",
      "[2022-12-30 15:38:31 +0000] [8163] [INFO] Worker exiting (pid: 8163)\n",
      "[2022-12-30 15:38:31 +0000] [8162] [INFO] Worker exiting (pid: 8162)\n",
      "[2022-12-30 15:38:31 +0000] [8157] [INFO] Worker exiting (pid: 8157)\n"
     ]
    }
   ],
   "source": [
    "! mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
