{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engineering.engineering import engineerFeatures\n",
    "from modelling.model_utils import train_model\n",
    "from utils.utils import gapfilling, serialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 18:02:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]: move config dictionnaries to a json config file\n",
    "features_config = {\n",
    "\n",
    "    \"discount_rate\":{},\n",
    "    \"promoted_percent\":{\"promoted_hierarchy\": \"sku\", \"group_key\":\"subclass\"},\n",
    "    \"week_of_year\":{},\n",
    "    \"promo_category\":{},\n",
    "}\n",
    "\n",
    "model_config={\n",
    "    \"model\": \"xgboost\",\n",
    "    \"params\": {},\n",
    "    \"hierarchy_columns\": [\"sku\", \"subclass\", \"store_id\", \"region_id\"],\n",
    "    \"target\": \"units\",\n",
    "    \"train_startDate\": \"2018-01-01\",\n",
    "    \"train_endDate\": \"2020-01-01\",\n",
    "    \"inference_startDate\": \"2019-11-01\",\n",
    "    \"inference_endDate\": \"2020-12-21\",\n",
    "}\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Union transcational data\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"week_index\", StringType(), True),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"promo_cat\", StringType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"store_id\", StringType(), True)],\n",
    ")\n",
    "\n",
    "transactions = spark.read.csv(\n",
    "    \"data/transactions_*.csv\", \n",
    "    schema=schema,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "customers = spark.read.csv(\n",
    "    \"data/customers.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "calendar = spark.read.csv(\n",
    "    \"data/calendar.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"data/products.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "stores = spark.read.csv(\n",
    "    \"data/stores.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding location hierarchy for customers\n",
    "\n",
    "customers = customers.select(\n",
    "    F.col(\"customer_id\").cast(\"string\"), \n",
    "    F.col(\"store_pref\").cast(\"string\").alias(\"store_id\")\n",
    ")\n",
    "\n",
    "stores = stores.select(\n",
    "    F.col(\"store_id\").cast(\"string\"), \n",
    "    F.col(\"store_region\").cast(\"string\").alias(\"region_id\")\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+----+----------+\n",
      "|sku|subclass|class|dept|base_price|\n",
      "+---+--------+-----+----+----------+\n",
      "|271|       0|    0|   7|    142.86|\n",
      "|289|       0|    0|   3|      20.0|\n",
      "|369|       0|    0|   2|     16.67|\n",
      "|410|       1|    1|  14|    525.33|\n",
      "|559|       0|    0|   4|     48.57|\n",
      "+---+--------+-----+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = products.select(\n",
    "    F.col(\"prod_id\").cast(\"string\").alias(\"sku\"),\n",
    "    F.col(\"prod_subclass\").cast(\"string\").alias(\"subclass\"),\n",
    "    F.col(\"prod_class\").cast(\"string\").alias(\"class\"),\n",
    "    F.col(\"prod_dept\").cast(\"string\").alias(\"dept\"),\n",
    "    F.col(\"prod_base_price\").cast(\"float\").alias(\"base_price\"),\n",
    ").dropDuplicates()\n",
    "\n",
    "products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|week_index|      date|\n",
      "+----------+----------+\n",
      "|         0|2018-01-01|\n",
      "|         1|2018-01-08|\n",
      "|         2|2018-01-15|\n",
      "|         3|2018-01-22|\n",
      "|         4|2018-01-29|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# daily calendar -> weekly calendar\n",
    "weekly_calendar = calendar.where(\n",
    "    F.col(\"day_of_week\")==\"0\"\n",
    ").select(\n",
    "    F.to_date(F.col(\"calendar_day\"),\"MM-dd-yy\").alias(\"date\")\n",
    ").distinct(\n",
    ").sort(\n",
    "    F.col(\"date\").asc()\n",
    ").withColumn(\n",
    "    \"week_index\", F.monotonically_increasing_id()\n",
    ").select(\n",
    "    F.col(\"week_index\").cast(\"string\"),\n",
    "    F.col(\"date\")\n",
    ")\n",
    "weekly_calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:44:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:44:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:44:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+--------+----------+\n",
      "|sku|store_id|units|promo_cat|discount|      date|\n",
      "+---+--------+-----+---------+--------+----------+\n",
      "|  0|      87|    1|      nan|    null|2020-02-03|\n",
      "|100|      83|    1|      nan|    null|2020-02-03|\n",
      "|101|      24|    1|      nan|    null|2020-02-03|\n",
      "|101|      55|    1|      nan|    null|2020-02-03|\n",
      "|102|      40|    1|      nan|    null|2020-02-03|\n",
      "+---+--------+-----+---------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add hierarchies\n",
    "demand_data = transactions.groupby(\n",
    "    \"sku\", \"store_id\", \"week_index\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"units\"),\n",
    "    F.first(\"promo_cat\").alias(\"promo_cat\"),\n",
    "    F.max(\"discount\").alias(\"discount\"),\n",
    ").join(\n",
    "    weekly_calendar, on=[\"week_index\"], how=\"inner\"\n",
    ").drop(\"week_index\")\n",
    "\n",
    "serialize(spark, demand_data, path + \"demand_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:44:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:44:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:45:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+-----+---------+--------+\n",
      "|      date|sku|store_id|units|promo_cat|discount|\n",
      "+----------+---+--------+-----+---------+--------+\n",
      "|2018-01-01|  0|      10|    0|     null|    null|\n",
      "|2018-01-01|  0|      20|    0|     null|    null|\n",
      "|2018-01-01|  0|      29|    0|     null|    null|\n",
      "|2018-01-01|  0|      34|    0|     null|    null|\n",
      "|2018-01-01|  0|      41|    0|     null|    null|\n",
      "+----------+---+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demand_data = spark.read.parquet(path + \"demand_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_filled_data = gapfilling(demand_data, date_column=\"date\", product_column=\"sku\", location_column=\"store_id\")\n",
    "\n",
    "serialize(spark, sales_filled_data, path + \"sales_filled_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:45:25 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/12/28 17:45:25 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/12/28 17:45:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 17:45:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "|sku|store_id|      date|units|promo_cat|discount|region_id|subclass|class|dept|base_price|\n",
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "|102|      11|2018-01-01|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "|102|       1|2018-01-22|    0|     null|    null|        2|       0|    0|   9|     164.0|\n",
      "|102|      11|2018-01-15|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "|102|       1|2018-02-26|    0|     null|    null|        2|       0|    0|   9|     164.0|\n",
      "|102|      11|2018-03-12|    0|     null|    null|        1|       0|    0|   9|     164.0|\n",
      "+---+--------+----------+-----+---------+--------+---------+--------+-----+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding product and location hierarchies to demand data\n",
    "demand_data = spark.read.parquet(path + \"sales_filled_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_data = demand_data.join(\n",
    "    stores, on=\"store_id\", how=\"inner\"\n",
    ").join(\n",
    "    products, on=\"sku\", how=\"inner\"\n",
    ")\n",
    "\n",
    "serialize(spark, sales_data, path + \"sales_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "|subclass|sku|store_id|      date|units|promo_cat|discount|region_id|class|dept|base_price|_discount_rate|_promoted_sku_subclass|_week_of_year|_promo_category|\n",
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "|       0|102|      11|2018-01-01|    0|     null|    null|        1|    0|   9|     164.0|           1.0|                   296|            1|           null|\n",
      "|       0|358|      41|2018-03-26|    0|     null|    null|        1|    0|   2|      15.0|           1.0|                   296|           13|           null|\n",
      "|       0|102|       1|2018-01-22|    0|     null|    null|        2|    0|   9|     164.0|           1.0|                   296|            4|           null|\n",
      "|       0|358|      37|2018-06-18|    0|     null|    null|        4|    0|   2|      15.0|           1.0|                   296|           25|           null|\n",
      "|       0|102|      11|2018-01-15|    0|     null|    null|        1|    0|   9|     164.0|           1.0|                   296|            3|           null|\n",
      "+--------+---+--------+----------+-----+---------+--------+---------+-----+----+----------+--------------+----------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = spark.read.parquet(path + \"sales_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "engineered_data = engineerFeatures(\n",
    "    data=sales_data,\n",
    "    config=features_config\n",
    ")\n",
    "\n",
    "serialize(spark, engineered_data, path + \"engineered_data.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-28 18:02:50.181724] Saving the train SPARK dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-28 18:02:57.631199] Reading the train dataframe using Pandas\n",
      "[2022-12-28 18:02:58.346298] One-hot encoding the train dataframe\n",
      "[2022-12-28 18:03:28.064621] Transforming the train one-hot encoded data into a CSR matrix\n",
      "[2022-12-28 18:04:41.285533] Saving the test SPARK dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-28 18:04:47.225439] Reading the test dataframe using Pandas\n",
      "[2022-12-28 18:04:47.658583] One-hot encoding the test dataframe\n",
      "[2022-12-28 18:04:57.403324] Transforming the test one-hot encoded data into a CSR matrix\n",
      "[2022-12-28 18:05:08.000476] mse=0.11511720835514748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>subclass</th>\n",
       "      <th>store_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>_week_of_year</th>\n",
       "      <th>_promo_category</th>\n",
       "      <th>_discount_rate</th>\n",
       "      <th>_promoted_sku_subclass</th>\n",
       "      <th>units</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>4</td>\n",
       "      <td>1.331852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399995</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399996</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>2</td>\n",
       "      <td>1.331852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399997</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399998</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>1.328338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399999</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sku subclass store_id region_id _week_of_year _promo_category  \\\n",
       "0        259        0       85         4            45             nan   \n",
       "1        259        0       39         1            39            None   \n",
       "2        102        0        1         2            47            None   \n",
       "3        102        0        1         2            51             nan   \n",
       "4        102        0        1         2             3            None   \n",
       "...      ...      ...      ...       ...           ...             ...   \n",
       "2399995    7        1       28         1            41            None   \n",
       "2399996    7        1       57         2            40             nan   \n",
       "2399997    7        1       28         1            42            None   \n",
       "2399998    7        1       57         2            50             nan   \n",
       "2399999    7        1       28         1            43            None   \n",
       "\n",
       "         _discount_rate  _promoted_sku_subclass  units  forecast  \n",
       "0                   1.0                     296      4  1.331852  \n",
       "1                   1.0                     296      0 -0.001409  \n",
       "2                   1.0                     296      0 -0.001042  \n",
       "3                   1.0                     296      1  1.203483  \n",
       "4                   1.0                     296      0 -0.001042  \n",
       "...                 ...                     ...    ...       ...  \n",
       "2399995             1.0                     290      0 -0.001409  \n",
       "2399996             1.0                     290      2  1.331852  \n",
       "2399997             1.0                     290      0 -0.001409  \n",
       "2399998             1.0                     290      1  1.328338  \n",
       "2399999             1.0                     290      0 -0.001409  \n",
       "\n",
       "[2400000 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered_data = spark.read.parquet(path + \"engineered_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "engineered_data = engineered_data.where(F.col(\"sku\")<400)\n",
    "\n",
    "train_model(engineered_data, model_config=model_config, features_config=features_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
