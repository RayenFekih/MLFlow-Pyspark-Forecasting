{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engineering.engineering import engineerFeatures\n",
    "from utils.utils import gapfilling, serialize\n",
    "from modelling.model_utils import splitData, prepare_data, train_model, MLFlow_train_model\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"test-app\")\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]: move config dictionnaries to a json config file\n",
    "features_config = {\n",
    "\n",
    "    \"discount_rate\":{},\n",
    "    \"promoted_percent\":{\"promoted_hierarchy\": \"sku\", \"group_key\":\"subclass\"},\n",
    "    \"week_of_year\":{},\n",
    "    \"promo_category\":{},\n",
    "}\n",
    "\n",
    "model_config={\n",
    "    \"model\": \"xgboost\",\n",
    "    \"params\": {},\n",
    "    \"hierarchy_columns\": [\"sku\", \"subclass\", \"store_id\", \"region_id\"],\n",
    "    \"target\": \"units\",\n",
    "    \"train_startDate\": \"2018-01-01\",\n",
    "    \"train_endDate\": \"2020-01-01\",\n",
    "    \"inference_startDate\": \"2019-11-01\",\n",
    "    \"inference_endDate\": \"2020-12-21\",\n",
    "}\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union transcational data\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"week_index\", StringType(), True),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"promo_cat\", StringType(), True),\n",
    "    StructField(\"discount\", FloatType(), True),\n",
    "    StructField(\"store_id\", StringType(), True)],\n",
    ")\n",
    "\n",
    "transactions = spark.read.csv(\n",
    "    \"data/transactions_*.csv\", \n",
    "    schema=schema,\n",
    "    header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "customers = spark.read.csv(\n",
    "    \"data/customers.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "calendar = spark.read.csv(\n",
    "    \"data/calendar.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "products = spark.read.csv(\n",
    "    \"data/products.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")\n",
    "\n",
    "stores = spark.read.csv(\n",
    "    \"data/stores.csv\", \n",
    "    header=\"true\", \n",
    "    inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding location hierarchy for customers\n",
    "\n",
    "customers = customers.select(\n",
    "    F.col(\"customer_id\").cast(\"string\"), \n",
    "    F.col(\"store_pref\").cast(\"string\").alias(\"store_id\")\n",
    ")\n",
    "\n",
    "stores = stores.select(\n",
    "    F.col(\"store_id\").cast(\"string\"), \n",
    "    F.col(\"store_region\").cast(\"string\").alias(\"region_id\")\n",
    ").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.select(\n",
    "    F.col(\"prod_id\").cast(\"string\").alias(\"sku\"),\n",
    "    F.col(\"prod_subclass\").cast(\"string\").alias(\"subclass\"),\n",
    "    F.col(\"prod_class\").cast(\"string\").alias(\"class\"),\n",
    "    F.col(\"prod_dept\").cast(\"string\").alias(\"dept\"),\n",
    "    F.col(\"prod_base_price\").cast(\"float\").alias(\"base_price\"),\n",
    ").dropDuplicates()\n",
    "\n",
    "products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily calendar -> weekly calendar\n",
    "weekly_calendar = calendar.where(\n",
    "    F.col(\"day_of_week\")==\"0\"\n",
    ").select(\n",
    "    F.to_date(F.col(\"calendar_day\"),\"MM-dd-yy\").alias(\"date\")\n",
    ").distinct(\n",
    ").sort(\n",
    "    F.col(\"date\").asc()\n",
    ").withColumn(\n",
    "    \"week_index\", F.monotonically_increasing_id()\n",
    ").select(\n",
    "    F.col(\"week_index\").cast(\"string\"),\n",
    "    F.col(\"date\")\n",
    ")\n",
    "weekly_calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hierarchies\n",
    "demand_data = transactions.groupby(\n",
    "    \"sku\", \"store_id\", \"week_index\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"units\"),\n",
    "    F.first(\"promo_cat\").alias(\"promo_cat\"),\n",
    "    F.max(\"discount\").alias(\"discount\"),\n",
    ").join(\n",
    "    weekly_calendar, on=[\"week_index\"], how=\"inner\"\n",
    ").drop(\"week_index\")\n",
    "\n",
    "serialize(spark, demand_data, path + \"demand_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_data = spark.read.parquet(path + \"demand_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_filled_data = gapfilling(demand_data, date_column=\"date\", product_column=\"sku\", location_column=\"store_id\")\n",
    "\n",
    "serialize(spark, sales_filled_data, path + \"sales_filled_data.parquet\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding product and location hierarchies to demand data\n",
    "demand_data = spark.read.parquet(path + \"sales_filled_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "sales_data = demand_data.join(\n",
    "    stores, on=\"store_id\", how=\"inner\"\n",
    ").join(\n",
    "    products, on=\"sku\", how=\"inner\"\n",
    ")\n",
    "\n",
    "serialize(spark, sales_data, path + \"sales_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = spark.read.parquet(path + \"sales_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "engineered_data = engineerFeatures(\n",
    "    data=sales_data,\n",
    "    config=features_config\n",
    ")\n",
    "\n",
    "serialize(spark, engineered_data, path + \"engineered_data.parquet\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_data = spark.read.parquet(path + \"engineered_data.parquet\", header=\"true\", inferSchema=\"true\")\n",
    "engineered_data = engineered_data.where(F.col(\"sku\")<400) # Just to reduce the size of the data for less memory consumption\n",
    "\n",
    "train_data, test_data = splitData(\n",
    "        data=engineered_data,\n",
    "        model_config=model_config,\n",
    "        features_config=features_config\n",
    "    )\n",
    "\n",
    "serialize(spark, train_data, path + \"train_data.parquet\").show(5)\n",
    "serialize(spark, test_data, path + \"test_data.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(f\"data/train_data.parquet\", \"pyarrow\")\n",
    "test_data = pd.read_parquet(f\"data/test_data.parquet\", \"pyarrow\")\n",
    "\n",
    "# preparing the training data\n",
    "X_train_ohe_sparse, y_train = prepare_data(\n",
    "    train_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"train\",\n",
    ")\n",
    "\n",
    "# preparing the inferencing data\n",
    "X_test_ohe_sparse, y_test = prepare_data(\n",
    "    test_data,\n",
    "    model_config,\n",
    "    features_config,\n",
    "    prefix=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal run\n",
    "train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow run\n",
    "\n",
    "max_depth_list = [3,4]\n",
    "learning_rate_list = [0.1, 0.001]\n",
    "n_estimators_list = [20, 25]\n",
    "\n",
    "for max_depth, learning_rate, n_estimators in zip(max_depth_list, learning_rate_list, n_estimators_list):\n",
    "    model_params = {\"max_depth\":max_depth, \"learning_rate\":learning_rate, \"n_estimators\":n_estimators}\n",
    "    MLFlow_train_model(X_train_ohe_sparse, X_test_ohe_sparse, y_train, y_test, model_config, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
